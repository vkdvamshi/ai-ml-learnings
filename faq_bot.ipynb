{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5fd19f0",
   "metadata": {},
   "source": [
    "Capstone: Personal FAQ Bot\n",
    "- Unlike generic ChatGPT, this bot only answers from your documents. No hallucination about your experience.\n",
    "- Retrieval-Augmented Generation: search your docs, find relevant chunks, generate accurate answers.\n",
    "- Deploy with Gradio for a link you can send to recruiters, clients, or colleagues.\n",
    "- This pattern scales to company knowledge bases, customer support, internal wikis.\n",
    "- Documents → Chunk → Embed → Store → Query → Retrieve → Generate → Answer\n",
    "\n",
    "Libraries & Dependencies: \n",
    "- pip install langchain langchain-openai chromadb gradio python-dotenv pypdf\n",
    "- OPENAI_API_KEY in .env file. Or use GROQ_API_KEY for free alternative.\n",
    "- Gather your resume, portfolio descriptions, project summaries as PDF or TXT files.\n",
    "- Single file works: faq_bot.py with docs/ folder for your documents.\n",
    "- langchain + chromadb + gradio + pypdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b46322b",
   "metadata": {},
   "source": [
    "- PyPDFLoader reads PDFs page by page. Works with resumes, reports, portfolios.\n",
    "- TextLoader for .txt and .md files. Good for project descriptions.\n",
    "- DirectoryLoader scans a folder and loads all matching files automatically.\n",
    "- Each document carries metadata (source file, page number) for citations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea8a7ef",
   "metadata": {},
   "source": [
    "\n",
    "vectorstore = Chroma.from_documents(chunks, OpenAIEmbeddings(), persist_directory=\"./chroma_db\") # Create and persist embedding db / vector store on local storage in choma_db\n",
    "\n",
    "This line is giving error: \n",
    "\n",
    "RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dac0dec",
   "metadata": {},
   "source": [
    "OpenAIEmbeddings are powerful machine learning models that convert text into high-dimensional vectors (numerical arrays) capturing deep semantic meaning. They power similarity searches, clustering, and Retrieval-Augmented Generation (RAG) by mapping semantically similar text close together in a vector space. Primarily used via langchain-openai in Python or JS, they support models like text-embedding-3 for efficient text, code, or image analysis. \n",
    "\n",
    "\n",
    "Key Aspects of OpenAIEmbeddings:\n",
    "\n",
    "Function: Converts text/documents into vectors, which are crucial for finding relationships in data.\n",
    "\n",
    "Best Use Cases: Semantic search, recommendations, data classification, and RAG applications.\n",
    "\n",
    "Model Options: Third-generation models (text-embedding-3-small, text-embedding-3-large) offer superior performance and configurable dimensions compared to ada-002.\n",
    "\n",
    "Limits: Maximum input is 8,191 tokens per request, with support for embedding batches of text.\n",
    "\n",
    "Integration: Used within the LangChain framework via from langchain_openai import OpenAIEmbeddings to easily embed documents for vector stores. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d940d803",
   "metadata": {},
   "source": [
    "What is langchain?\n",
    "\n",
    "LangChain: An open-source framework that helps you orchestrate the interaction between LLMs, vector stores, embedding models, etc, making it easier to integrate a RAG pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cfd089",
   "metadata": {},
   "source": [
    "https://groq.com/blog/retrieval-augmented-generation-with-groq-api\n",
    "\n",
    "https://cdn.sanity.io/images/chol0sk5/production/d9fa5305217baf2ec61d2d064519840c9218ac9b-1012x748.png\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1937495e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 1] Operation not permitted",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPermissionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 87\u001b[39m\n\u001b[32m     78\u001b[39m retriever = vectorstore.as_retriever(search_kwargs={\u001b[33m\"\u001b[39m\u001b[33mk\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m4\u001b[39m})\n\u001b[32m     79\u001b[39m qa_chain = RetrievalQA.from_chain_type(llm=ChatOpenAI(temperature=\u001b[32m0\u001b[39m),chain_type=\u001b[33m\"\u001b[39m\u001b[33mstuff\u001b[39m\u001b[33m\"\u001b[39m,retriever=retriever,chain_type_kwargs={\n\u001b[32m     80\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mverbose\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     81\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m: PROMPT,\n\u001b[32m   (...)\u001b[39m\u001b[32m     84\u001b[39m             input_key=\u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     85\u001b[39m     }, return_source_documents=\u001b[38;5;28;01mTrue\u001b[39;00m,verbose=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m \u001b[43mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mChatInterface\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mAsk About [Your Name]\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m.launch()\n\u001b[32m     89\u001b[39m \u001b[38;5;66;03m# query = \"What is the summary of the document?\"\u001b[39;00m\n\u001b[32m     90\u001b[39m \u001b[38;5;66;03m# result = qa_chain({\"query\": query})\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[38;5;66;03m# print (result['result'])\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Python/3.12/lib/python/site-packages/gradio/chat_interface.py:146\u001b[39m, in \u001b[36mChatInterface.__init__\u001b[39m\u001b[34m(self, fn, multimodal, chatbot, textbox, additional_inputs, additional_inputs_accordion, additional_outputs, editable, examples, example_labels, example_icons, run_examples_on_click, cache_examples, cache_mode, title, description, flagging_mode, flagging_options, flagging_dir, analytics_enabled, autofocus, autoscroll, submit_btn, stop_btn, concurrency_limit, delete_cache, show_progress, fill_height, fill_width, api_name, api_description, api_visibility, save_history, validator)\u001b[39m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m     72\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     73\u001b[39m     fn: Callable,\n\u001b[32m   (...)\u001b[39m\u001b[32m    107\u001b[39m     validator: Callable | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    108\u001b[39m ):\n\u001b[32m    109\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    110\u001b[39m \u001b[33;03m    Parameters:\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m        fn: the function to wrap the chat interface around. The function should accept two parameters: a `str` representing the input message and `list` of openai-style dictionaries: {\"role\": \"user\" | \"assistant\", \"content\": `str` | {\"path\": `str`} | `gr.Component`} representing the chat history. The function should return/yield a `str` (for a simple message), a supported Gradio component (e.g. gr.Image to return an image), a `dict` (for a complete openai-style message response), or a `list` of such messages.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    144\u001b[39m \u001b[33;03m        validator: a function that takes in the inputs and can optionally return a gr.validate() object for each input.\u001b[39;00m\n\u001b[32m    145\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[43m        \u001b[49m\u001b[43manalytics_enabled\u001b[49m\u001b[43m=\u001b[49m\u001b[43manalytics_enabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mchat_interface\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGradio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfill_height\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfill_height\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfill_width\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfill_width\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    152\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdelete_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdelete_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    154\u001b[39m     \u001b[38;5;28mself\u001b[39m.api_name: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = api_name\n\u001b[32m    155\u001b[39m     \u001b[38;5;28mself\u001b[39m.api_description: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m | Literal[\u001b[38;5;28;01mFalse\u001b[39;00m] = api_description\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Python/3.12/lib/python/site-packages/gradio/blocks.py:1164\u001b[39m, in \u001b[36mBlocks.__init__\u001b[39m\u001b[34m(self, analytics_enabled, mode, title, fill_height, fill_width, delete_cache, **kwargs)\u001b[39m\n\u001b[32m   1160\u001b[39m     analytics.initiated_analytics(data)\n\u001b[32m   1162\u001b[39m Blocks.instances.add(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1164\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Python/3.12/lib/python/site-packages/gradio/blocks.py:2403\u001b[39m, in \u001b[36mBlocks.queue\u001b[39m\u001b[34m(self, status_update_rate, api_open, max_size, default_concurrency_limit)\u001b[39m\n\u001b[32m   2394\u001b[39m     max_size = \u001b[32m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m max_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m max_size\n\u001b[32m   2395\u001b[39m \u001b[38;5;28mself\u001b[39m._queue = queueing.Queue(\n\u001b[32m   2396\u001b[39m     live_updates=status_update_rate == \u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2397\u001b[39m     concurrency_count=\u001b[38;5;28mself\u001b[39m.max_threads,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2401\u001b[39m     default_concurrency_limit=default_concurrency_limit,\n\u001b[32m   2402\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2403\u001b[39m \u001b[38;5;28mself\u001b[39m.app = \u001b[43mApp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_app\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmcp_server\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   2404\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Python/3.12/lib/python/site-packages/gradio/routes.py:439\u001b[39m, in \u001b[36mApp.create_app\u001b[39m\u001b[34m(blocks, app_kwargs, auth_dependency, strict_cors, ssr_mode, mcp_server)\u001b[39m\n\u001b[32m    436\u001b[39m     blocks.mcp_server_obj.launch_mcp_on_sse(app, mcp_subpath, blocks.root_path)\n\u001b[32m    437\u001b[39m router = APIRouter(prefix=API_PREFIX)\n\u001b[32m--> \u001b[39m\u001b[32m439\u001b[39m \u001b[43mapp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfigure_app\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblocks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    441\u001b[39m app.add_middleware(CustomCORSMiddleware, strict_cors=strict_cors)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    442\u001b[39m app.add_middleware(\n\u001b[32m    443\u001b[39m     BrotliMiddleware,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    444\u001b[39m     quality=\u001b[32m4\u001b[39m,\n\u001b[32m    445\u001b[39m     excluded_handlers=[mcp_subpath],\n\u001b[32m    446\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Python/3.12/lib/python/site-packages/gradio/routes.py:340\u001b[39m, in \u001b[36mApp.configure_app\u001b[39m\u001b[34m(self, blocks)\u001b[39m\n\u001b[32m    338\u001b[39m     \u001b[38;5;28mself\u001b[39m.auth = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    339\u001b[39m \u001b[38;5;28mself\u001b[39m.blocks = blocks\n\u001b[32m--> \u001b[39m\u001b[32m340\u001b[39m \u001b[38;5;28mself\u001b[39m.cwd = \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetcwd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[38;5;28mself\u001b[39m.favicon_path = blocks.favicon_path\n\u001b[32m    342\u001b[39m \u001b[38;5;28mself\u001b[39m.tokens = {}\n",
      "\u001b[31mPermissionError\u001b[39m: [Errno 1] Operation not permitted"
     ]
    }
   ],
   "source": [
    "from unittest import loader\n",
    "from webbrowser import Chrome\n",
    "import chromadb\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_classic.chains.query_constructor.base import AttributeInfo\n",
    "# from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_classic.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "# from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_aws import BedrockEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_classic.chains.query_constructor.base import AttributeInfo\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_classic.retrievers import SelfQueryRetriever\n",
    "from pinecone import Pinecone\n",
    "from langchain_classic.document_loaders import DirectoryLoader\n",
    "from langchain_classic.chains import RetrievalQA\n",
    "import gradio as gr\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_classic.chains import ConversationalRetrievalChain\n",
    "from langchain_classic.memory import ConversationBufferMemory\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_classic.vectorstores import FAISS\n",
    "import os \n",
    "FAISS_INDEX_PATH = \"faiss_index\"\n",
    "import streamlit as st\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader \n",
    "from langchain_openai import ChatOpenAI  \n",
    "from langchain_classic.schema import AIMessage, HumanMessage  \n",
    "import gradio as gr\n",
    "### prompts\n",
    "from langchain_classic import PromptTemplate, LLMChain\n",
    "\n",
    "# Groq API endpoint for embeddings (example model: nomic-embed-text)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "# embeddings = GroqEmbeddings(model=\"text-embedding-3-large\") # didn't work due to missing methods of embeddings.\n",
    "# llm = OpenAI(temperature=0)\n",
    "# llm = ChatGroq(model=\"deepseek-r1-distill-llama-70b\",temperature=0.7,max_tokens=500)\n",
    "llm = init_chat_model(\"qwen-2.5-32b\", model_provider=\"groq\")\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "def chat(msg, chat_history=[]):\n",
    "    return qa_chain({\"query\": msg} )['result']\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "Don't try to make up an answer, if you don't know just say that you don't know.\n",
    "Answer in the same language the question was asked.\n",
    "Use only the following pieces of context to answer the question at the end.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template = prompt_template, \n",
    "    input_variables = [\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# vectorstore = Chroma.from_documents(chunks, BedrockEmbeddings(model=\"amazon.titan-embed-text-v1\"), persist_directory=\"./chroma_db\") # Create and persist embedding db / vector store on local storage in choma_db\n",
    "# vectorstore = InMemoryVectorStore(chunks, embeddings)\n",
    "# vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "# demo = gr.ChatInterface(\n",
    "#     answer,\n",
    "#     title=\"Llama Index RAG Chatbot\",\n",
    "# ).launch()\n",
    "documents = DirectoryLoader(\"/Users/vkdvamshi/Resumes/\", glob=\"**/*.pdf\").load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunks = text_splitter.split_documents(documents) # Split documents into chunks\n",
    "vectorstore = Chroma.from_documents(chunks, embeddings) # Create and persist embedding db / vector store on local storage in choma_db\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=ChatOpenAI(temperature=0),chain_type=\"stuff\",retriever=retriever,chain_type_kwargs={\n",
    "        \"verbose\": True,\n",
    "        \"prompt\": PROMPT,\n",
    "        \"memory\": ConversationBufferMemory(\n",
    "            memory_key=\"history\",\n",
    "            input_key=\"question\"),\n",
    "    }, return_source_documents=True,verbose=True)\n",
    "\n",
    "gr.ChatInterface(fn=chat, title=\"Ask About [Your Name]\").launch(share=True)\n",
    "\n",
    "# query = \"What is the summary of the document?\"\n",
    "# result = qa_chain({\"query\": query})\n",
    "# print (result['result'])\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
