{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5fd19f0",
   "metadata": {},
   "source": [
    "Capstone: Personal FAQ Bot\n",
    "- Unlike generic ChatGPT, this bot only answers from your documents. No hallucination about your experience.\n",
    "- Retrieval-Augmented Generation: search your docs, find relevant chunks, generate accurate answers.\n",
    "- Deploy with Gradio for a link you can send to recruiters, clients, or colleagues.\n",
    "- This pattern scales to company knowledge bases, customer support, internal wikis.\n",
    "- Documents → Chunk → Embed → Store → Query → Retrieve → Generate → Answer\n",
    "\n",
    "Libraries & Dependencies: \n",
    "- pip install langchain langchain-openai chromadb gradio python-dotenv pypdf\n",
    "- OPENAI_API_KEY in .env file. Or use GROQ_API_KEY for free alternative.\n",
    "- Gather your resume, portfolio descriptions, project summaries as PDF or TXT files.\n",
    "- Single file works: faq_bot.py with docs/ folder for your documents.\n",
    "- langchain + chromadb + gradio + pypdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b46322b",
   "metadata": {},
   "source": [
    "- PyPDFLoader reads PDFs page by page. Works with resumes, reports, portfolios.\n",
    "- TextLoader for .txt and .md files. Good for project descriptions.\n",
    "- DirectoryLoader scans a folder and loads all matching files automatically.\n",
    "- Each document carries metadata (source file, page number) for citations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea8a7ef",
   "metadata": {},
   "source": [
    "\n",
    "vectorstore = Chroma.from_documents(chunks, OpenAIEmbeddings(), persist_directory=\"./chroma_db\") # Create and persist embedding db / vector store on local storage in choma_db\n",
    "\n",
    "This line is giving error: \n",
    "\n",
    "RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dac0dec",
   "metadata": {},
   "source": [
    "OpenAIEmbeddings are powerful machine learning models that convert text into high-dimensional vectors (numerical arrays) capturing deep semantic meaning. They power similarity searches, clustering, and Retrieval-Augmented Generation (RAG) by mapping semantically similar text close together in a vector space. Primarily used via langchain-openai in Python or JS, they support models like text-embedding-3 for efficient text, code, or image analysis. \n",
    "\n",
    "\n",
    "Key Aspects of OpenAIEmbeddings:\n",
    "\n",
    "Function: Converts text/documents into vectors, which are crucial for finding relationships in data.\n",
    "\n",
    "Best Use Cases: Semantic search, recommendations, data classification, and RAG applications.\n",
    "\n",
    "Model Options: Third-generation models (text-embedding-3-small, text-embedding-3-large) offer superior performance and configurable dimensions compared to ada-002.\n",
    "\n",
    "Limits: Maximum input is 8,191 tokens per request, with support for embedding batches of text.\n",
    "\n",
    "Integration: Used within the LangChain framework via from langchain_openai import OpenAIEmbeddings to easily embed documents for vector stores. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d940d803",
   "metadata": {},
   "source": [
    "What is langchain?\n",
    "\n",
    "LangChain: An open-source framework that helps you orchestrate the interaction between LLMs, vector stores, embedding models, etc, making it easier to integrate a RAG pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cfd089",
   "metadata": {},
   "source": [
    "https://groq.com/blog/retrieval-augmented-generation-with-groq-api\n",
    "\n",
    "https://cdn.sanity.io/images/chol0sk5/production/d9fa5305217baf2ec61d2d064519840c9218ac9b-1012x748.png\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1937495e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unittest import loader\n",
    "from webbrowser import Chrome\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_classic.chains.query_constructor.base import AttributeInfo\n",
    "# from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_classic.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "# from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_aws import BedrockEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_classic.chains.query_constructor.base import AttributeInfo\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_classic.retrievers import SelfQueryRetriever\n",
    "from pinecone import Pinecone\n",
    "from langchain_classic.document_loaders import DirectoryLoader\n",
    "from langchain_classic.chains import RetrievalQA\n",
    "import gradio as gr\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_classic.chains import ConversationalRetrievalChain\n",
    "from langchain_classic.memory import ConversationBufferMemory\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_classic.vectorstores import FAISS\n",
    "import os \n",
    "FAISS_INDEX_PATH = \"faiss_index\"\n",
    "import streamlit as st\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader \n",
    "from langchain_openai import ChatOpenAI  \n",
    "from langchain_classic.schema import AIMessage, HumanMessage  \n",
    "import gradio as gr\n",
    "### prompts\n",
    "from langchain_classic import PromptTemplate, LLMChain\n",
    "\n",
    "# Groq API endpoint for embeddings (example model: nomic-embed-text)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "# embeddings = GroqEmbeddings(model=\"text-embedding-3-large\") # didn't work due to missing methods of embeddings.\n",
    "# llm = OpenAI(temperature=0)\n",
    "# llm = ChatGroq(model=\"deepseek-r1-distill-llama-70b\",temperature=0.7,max_tokens=500)\n",
    "llm = init_chat_model(\"qwen-2.5-32b\", model_provider=\"groq\")\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "def chat(msg, chat_history=[]):\n",
    "    return qa_chain({\"query\": msg} )['result']\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "Don't try to make up an answer, if you don't know just say that you don't know.\n",
    "Answer in the same language the question was asked.\n",
    "Use only the following pieces of context to answer the question at the end.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template = prompt_template, \n",
    "    input_variables = [\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# vectorstore = Chroma.from_documents(chunks, BedrockEmbeddings(model=\"amazon.titan-embed-text-v1\"), persist_directory=\"./chroma_db\") # Create and persist embedding db / vector store on local storage in choma_db\n",
    "# vectorstore = InMemoryVectorStore(chunks, embeddings)\n",
    "# vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "# demo = gr.ChatInterface(\n",
    "#     answer,\n",
    "#     title=\"Llama Index RAG Chatbot\",\n",
    "# ).launch()\n",
    "documents = DirectoryLoader(\"/Users/vkdvamshi/Resumes/\", glob=\"**/*.pdf\").load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunks = text_splitter.split_documents(documents) # Split documents into chunks\n",
    "vectorstore = Chroma.from_documents(chunks, embeddings, persist_directory=\"./chroma_db\")\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=ChatOpenAI(temperature=0),chain_type=\"stuff\",retriever=retriever,chain_type_kwargs={\n",
    "        \"verbose\": True,\n",
    "        \"prompt\": PROMPT,\n",
    "        \"memory\": ConversationBufferMemory(\n",
    "            memory_key=\"history\",\n",
    "            input_key=\"question\"),\n",
    "    }, return_source_documents=True,verbose=True)\n",
    "\n",
    "gr.ChatInterface(fn=chat, title=\"Ask About [Your Name]\").launch()\n",
    "\n",
    "# query = \"What is the summary of the document?\"\n",
    "# result = qa_chain({\"query\": query})\n",
    "# print (result['result'])\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
