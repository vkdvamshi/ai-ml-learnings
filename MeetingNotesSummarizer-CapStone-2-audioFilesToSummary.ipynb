{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fcc4af0",
   "metadata": {},
   "source": [
    "-Libraries :  langchain langchain-openai streamlit python-dotenv\n",
    "\n",
    "Streamlit is a Python library used to create interactive web applications for data science and machine learning. It is often used to build AI-powered meeting notes summarizers. These summarizers allow for the quick deployment of interfaces that can upload audio or text files, interact with large language models, and display structured summaries. \n",
    "Here is an overview of how to build or find Streamlit-based meeting summarizers:\n",
    "Key Features of Streamlit Meeting Summarizers\n",
    "\n",
    "File Uploading: Supports uploading audio (.mp3, .wav) or text (.txt, .pdf, .docx) files.\n",
    "\n",
    "Transcription: Integrates with APIs like AssemblyAI or OpenAI Whisper to convert audio to text.\n",
    "\n",
    "AI Summarization: Uses LangChain with models like GPT-4 or Gemini-1.5-flash to generate summaries, action items, and key takeaways.\n",
    "\n",
    "Download Options: Allows users to export summaries as PDF, Word documents, or Markdown. \n",
    "\n",
    "Examples of Streamlit Apps\n",
    "Meeting Notes Summarizer & Sharer (Groq): A tool that accepts text transcripts and generates bullet points for executives.\n",
    "\n",
    "Meeting Minutes Extractor: Uses OpenAIs API to turn transcripts into structured markdown notes.\n",
    "\n",
    "AI Meeting Summarizer (Assembly AI): Allows users to upload audio files, which are then transcribed and divided into chapters with summaries.\n",
    "\n",
    "Live Meeting Summarizer: Provides speaker diarization and summarizes the conversation. \n",
    "Example Implementation (Backend Structure)\n",
    "\n",
    "A typical Streamlit app for this purpose often involves:\n",
    "\n",
    "UI Setup: import streamlit as st\n",
    "\n",
    "File Loading: st.file_uploader()\n",
    "\n",
    "Backend Integration: Using LangChain for prompt engineering and LLM interaction.\n",
    "\n",
    "Display: st.markdown() or st.text_area() to show results. \n",
    "Common Tech Stack\n",
    "Frontend/App Framework: Streamlit\n",
    "\n",
    "LLM API: OpenAI (GPT-4o), Google Gemini, or Groq\n",
    "\n",
    "Orchestration: LangChain\n",
    "\n",
    "Speech-to-Text: Whisper or AssemblyAI \n",
    "These apps can be easily deployed on the Streamlit Community Cloud. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4df8c424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7877\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7877/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    }
   ],
   "source": [
    "#https://github.com/AlexisBalayre/AI-Powered-Meeting-Summarizer\n",
    "from asyncio import subprocess\n",
    "import requests\n",
    "import streamlit as st\n",
    "import gradio as gr\n",
    "import json\n",
    "import os\n",
    "\n",
    "OLLAMA_SERVER_URL = \"http://localhost:11434\"  # Replace this with your actual Ollama server URL if different\n",
    "WHISPER_MODEL_DIR = \"/Users/vkdvamshi/gitRepos/whisper.cpp/models\"  # Directory where whisper models are stored\n",
    "\n",
    "def get_available_models() -> list[str]:\n",
    "    \"\"\"\n",
    "    Retrieves a list of all available models from the Ollama server and extracts the model names.\n",
    "\n",
    "    Returns:\n",
    "        A list of model names available on the Ollama server.\n",
    "    \"\"\"\n",
    "    response = requests.get(f\"{OLLAMA_SERVER_URL}/api/tags\")\n",
    "    if response.status_code == 200:\n",
    "        models = response.json()[\"models\"]\n",
    "        llm_model_names = [model[\"model\"] for model in models]  # Extract model names\n",
    "        return llm_model_names\n",
    "    else:\n",
    "        raise Exception(\n",
    "            f\"Failed to retrieve models from Ollama server: {response.text}\"\n",
    "        )\n",
    "\n",
    "\n",
    "def get_available_whisper_models() -> list[str]:\n",
    "    \"\"\"\n",
    "    Retrieves a list of available Whisper models based on downloaded .bin files in the whisper.cpp/models directory.\n",
    "    Filters out test models and only includes official Whisper models (e.g., base, small, medium, large).\n",
    "\n",
    "    Returns:\n",
    "        A list of available Whisper model names (e.g., 'base', 'small', 'medium', 'large-V3').\n",
    "    \"\"\"\n",
    "    # List of acceptable official Whisper models\n",
    "    valid_models = [\"base\", \"small\", \"medium\", \"large\", \"large-V3\"]\n",
    "\n",
    "    # Get the list of model files in the models directory\n",
    "    model_files = [f for f in os.listdir(WHISPER_MODEL_DIR) if f.endswith(\".bin\")]\n",
    "\n",
    "    # Filter out test models and models that aren't in the valid list\n",
    "    whisper_models = [\n",
    "        os.path.splitext(f)[0].replace(\"ggml-\", \"\")\n",
    "        for f in model_files\n",
    "        if any(valid_model in f for valid_model in valid_models) and \"test\" not in f\n",
    "    ]\n",
    "\n",
    "    # Remove any potential duplicates\n",
    "    whisper_models = list(set(whisper_models))\n",
    "    return whisper_models\n",
    "\n",
    "def summarize_with_model(llm_model_name: str, context: str, text: str) -> str:\n",
    "    \"\"\"\n",
    "    Uses a specified model on the Ollama server to generate a summary.\n",
    "    Handles streaming responses by processing each line of the response.\n",
    "\n",
    "    Args:\n",
    "        llm_model_name (str): The name of the model to use for summarization.\n",
    "        context (str): Optional context for the summary, provided by the user.\n",
    "        text (str): The transcript text to summarize.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated summary text from the model.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"You are given a transcript from a meeting, along with some optional context.\n",
    "    \n",
    "    Context: {context if context else 'No additional context provided.'}\n",
    "    \n",
    "    The transcript is as follows:\n",
    "    \n",
    "    {text}\n",
    "    \n",
    "    Please summarize the transcript.\"\"\"\n",
    "\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    data = {\"model\": llm_model_name, \"prompt\": prompt}\n",
    "\n",
    "    response = requests.post(f\"{OLLAMA_SERVER_URL}/api/generate\", json=data, headers=headers, stream=True\n",
    "    )\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        full_response = \"\"\n",
    "        try:\n",
    "            # Process the streaming response line by line\n",
    "            for line in response.iter_lines():\n",
    "                if line:\n",
    "                    # Decode each line and parse it as a JSON object\n",
    "                    decoded_line = line.decode(\"utf-8\")\n",
    "                    json_line = json.loads(decoded_line)\n",
    "                    # Extract the \"response\" part from each JSON object\n",
    "                    full_response += json_line.get(\"response\", \"\")\n",
    "                    # If \"done\" is True, break the loop\n",
    "                    if json_line.get(\"done\", False):\n",
    "                        break\n",
    "            return full_response\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"Error: Response contains invalid JSON data.\")\n",
    "            return f\"Failed to parse the response from the server. Raw response: {response.text}\"\n",
    "    else:\n",
    "        raise Exception(\n",
    "            f\"Failed to summarize with model {llm_model_name}: {response.text}\"\n",
    "        )\n",
    "        \n",
    "def preprocess_audio_file(audio_file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Converts the input audio file to a WAV format with 16kHz sample rate and mono channel.\n",
    "\n",
    "    Args:\n",
    "        audio_file_path (str): Path to the input audio file.\n",
    "\n",
    "    Returns:\n",
    "        str: The path to the preprocessed WAV file.\n",
    "    \"\"\"\n",
    "    output_wav_file = f\"{os.path.splitext(audio_file_path)[0]}_converted.wav\"\n",
    "\n",
    "    # Ensure ffmpeg converts to 16kHz sample rate and mono channel\n",
    "    cmd = f'ffmpeg -y -i \"{audio_file_path}\" -ar 16000 -ac 1 \"{output_wav_file}\"'\n",
    "    subprocess.run(cmd, shell=True, check=True)\n",
    "\n",
    "    return output_wav_file\n",
    "\n",
    "def translate_and_summarize(\n",
    "    audio_file_path: str, context: str, whisper_model_name: str, llm_model_name: str\n",
    ") -> tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Translates the audio file into text using the whisper.cpp model and generates a summary using Ollama.\n",
    "    Also provides the transcript file for download.\n",
    "\n",
    "    Args:\n",
    "        audio_file_path (str): Path to the input audio file.\n",
    "        context (str): Optional context to include in the summary.\n",
    "        whisper_model_name (str): Whisper model to use for audio-to-text conversion.\n",
    "        llm_model_name (str): Model to use for summarizing the transcript.\n",
    "\n",
    "    Returns:\n",
    "        tuple[str, str]: A tuple containing the summary and the path to the transcript file for download.\n",
    "    \"\"\"\n",
    "    output_file = \"output.txt\"\n",
    "\n",
    "    print(\"Processing audio file:\", audio_file_path)\n",
    "\n",
    "    # Convert the input file to WAV format if necessary\n",
    "    audio_file_wav = preprocess_audio_file(audio_file_path)\n",
    "\n",
    "    print(\"Audio preprocessed:\", audio_file_wav)\n",
    "\n",
    "    # Call the whisper.cpp binary\n",
    "    whisper_command = f'./whisper.cpp/main -m ./whisper.cpp/models/ggml-{whisper_model_name}.bin -f \"{audio_file_wav}\" > {output_file}'\n",
    "    subprocess.run(whisper_command, shell=True, check=True)\n",
    "\n",
    "    print(\"Whisper.cpp executed successfully\")\n",
    "\n",
    "    # Read the output from the transcript\n",
    "    with open(output_file, \"r\") as f:\n",
    "        transcript = f.read()\n",
    "\n",
    "    # Save the transcript to a downloadable file\n",
    "    transcript_file = \"transcript.txt\"\n",
    "    with open(transcript_file, \"w\") as transcript_f:\n",
    "        transcript_f.write(transcript)\n",
    "\n",
    "    # Generate summary from the transcript using Ollama's model\n",
    "    summary = summarize_with_model(llm_model_name, context, transcript)\n",
    "\n",
    "    # Clean up temporary files\n",
    "    os.remove(audio_file_wav)\n",
    "    os.remove(output_file)\n",
    "\n",
    "    # Return the downloadable link for the transcript and the summary text\n",
    "    return summary, transcript_file\n",
    "\n",
    "# Gradio interface\n",
    "def gradio_app(\n",
    "    audio, context: str, whisper_model_name: str, llm_model_name: str\n",
    ") -> tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Gradio application to handle file upload, model selection, and summary generation.\n",
    "\n",
    "    Args:\n",
    "        audio: The uploaded audio file.\n",
    "        context (str): Optional context provided by the user.\n",
    "        whisper_model_name (str): The selected Whisper model name.\n",
    "        llm_model_name (str): The selected language model for summarization.\n",
    "\n",
    "    Returns:\n",
    "        tuple[str, str]: A tuple containing the summary text and a downloadable transcript file.\n",
    "    \"\"\"\n",
    "    return translate_and_summarize(audio, context, whisper_model_name, llm_model_name)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Retrieve available models for Gradio dropdown input\n",
    "    ollama_models = get_available_models()  # Retrieve models from Ollama server\n",
    "    whisper_models = (\n",
    "        get_available_whisper_models()\n",
    "    )  # Dynamically detect downloaded Whisper models\n",
    "\n",
    "    iface = gr.Interface(\n",
    "        fn=gradio_app,\n",
    "        inputs=[\n",
    "            gr.Audio(type=\"filepath\", label=\"Upload an audio file\"),\n",
    "            gr.Textbox(\n",
    "                label=\"Context (optional)\",\n",
    "                placeholder=\"Provide any additional context for the summary\",\n",
    "            ),\n",
    "            gr.Dropdown(\n",
    "                choices=whisper_models,\n",
    "                label=\"Select a Whisper model for audio-to-text conversion\",\n",
    "                value=whisper_models[0],\n",
    "            ),\n",
    "            gr.Dropdown(\n",
    "                choices=ollama_models,\n",
    "                label=\"Select a model for summarization\",\n",
    "                value=ollama_models[0] if ollama_models else None,\n",
    "            ),\n",
    "        ],\n",
    "        outputs=[\n",
    "            gr.Textbox(\n",
    "                label=\"Summary\",\n",
    "                # show_copy_button=True,\n",
    "                # show_label=True,\n",
    "            ),  # Display the summary generated by the Ollama model\n",
    "            gr.File(\n",
    "                label=\"Download Transcript\"\n",
    "            ),  # Provide the transcript as a downloadable file\n",
    "        ],\n",
    "        analytics_enabled=False,\n",
    "        title=\"Meeting Summarizer\",\n",
    "        description=\"Upload an audio file of a meeting and get a summary of the key concepts discussed.\",\n",
    "    )\n",
    "    \n",
    "    iface.launch(debug=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
